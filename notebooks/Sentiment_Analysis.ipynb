{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "Using: https://towardsdatascience.com/how-to-analyze-emotions-and-words-of-the-lyrics-from-your-favorite-music-artist-bbca10411283"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#download everything\n",
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run helpers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries used to extract, clean and manipulate the data\n",
    "from helpers import *\n",
    "import pandas as pd\n",
    "import os\n",
    "#To plot the graphs\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "#library used to count the frequency of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#To create the sentiment analysis model, tokenization and lemmatization\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk.data\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export folder path\n",
    "export_path = os.path.join(\"..\", \"datasets\", \"SentimentAnalysis\")\n",
    "if not os.path.exists(export_path):\n",
    "  os.makedirs(export_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load data\n",
    "#import data\n",
    "df2017 = pd.read_csv ('../datasets/CleanData/2017_cleaned_songs_lyrics.csv')\n",
    "\n",
    "df2018 = pd.read_csv ('../datasets/CleanData/2018_cleaned_songs_lyrics.csv')\n",
    "\n",
    "df2019 = pd.read_csv ('../datasets/CleanData/2019_cleaned_songs_lyrics.csv')\n",
    "\n",
    "df2020 = pd.read_csv ('../datasets/CleanData/2020_cleaned_songs_lyrics.csv')\n",
    "\n",
    "df2021 = pd.read_csv ('../datasets/CleanData/2021_cleaned_songs_lyrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key things to note for helpers.py\n",
    "- cleaning lyrics: changed all words to lowercase, removed extra characters\n",
    "- lyrics to words function: removed stopwords, lemmatized, removed punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2017['Year'] = 2017\n",
    "df2017 = df2017.iloc[:,1:]\n",
    "df2018['Year'] = 2018\n",
    "df2018 = df2018.iloc[:,1:]\n",
    "df2019['Year'] = 2019\n",
    "df2019 = df2019.iloc[:,1:]\n",
    "df2020['Year'] = 2020\n",
    "df2020 = df2020.iloc[:,1:]\n",
    "df2021['Year'] = 2021\n",
    "df2021 = df2021.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2019.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df2017\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating unique words and examining them (Year 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Year'] = 2017\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(list1):\n",
    "   # intilize a null list\n",
    "     unique_list = []\n",
    "   # traverse for all elements\n",
    "     for x in list1:\n",
    "         # check if exists in unique_list or not\n",
    "         if x not in unique_list:\n",
    "              unique_list.append(x)\n",
    "     return unique_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stores unique words of each lyrics song into a new column called words\n",
    "#list used to store the words\n",
    "words = []\n",
    "\n",
    "#iterate trought each lyric and split unique words appending the result into the words list\n",
    "df = df.reset_index(drop=True)\n",
    "for word in df['lemmatized_lyrics'].tolist():\n",
    "    words.append(unique(lyrics_to_words(word).split()))\n",
    "    \n",
    "#create the new column with the information of words lists\n",
    "df['words'] = words\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new dataframe of all the  words used in lyrics and its decades\n",
    "#list used to store the information\n",
    "set_words = []\n",
    "set_year = []\n",
    "\n",
    "#Iterate trought each word and decade and stores them into the new lists\n",
    "for i in df.index:\n",
    "    for word in df['words'].iloc[i]:\n",
    "        set_words.append(word)\n",
    "        set_year.append(df['Year'].iloc[i])\n",
    "        \n",
    "#create the new data frame  with the information of words and decade lists\n",
    "words_df = pd.DataFrame({'words':set_words,'Year':set_year})\n",
    "\n",
    "# count the frequency of each word that aren't on the stop_words lists\n",
    "cv = CountVectorizer()\n",
    "\n",
    "#Create a dataframe called data_cv to store the the number of times the word was used in  a lyric based their decades\n",
    "text_cv = cv.fit_transform(words_df['words'].iloc[:])\n",
    "data_cv = pd.DataFrame(text_cv.toarray(),columns=cv.get_feature_names())\n",
    "data_cv['Year'] = words_df['Year']\n",
    "\n",
    "#created a dataframe that Sums the ocurrence frequency of each word and group the result by decade\n",
    "vect_words = data_cv.groupby('Year').sum().T\n",
    "vect_words = vect_words.reset_index(level=0).rename(columns ={'index':'words'})\n",
    "vect_words = vect_words.rename_axis(columns='')\n",
    "\n",
    "#Save the data into a csv file\n",
    "vect_words.to_csv(os.path.join(export_path, 'words2017.csv'), index=False)\n",
    "vect_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating unique words and examining them (Year 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stores unique words of each lyrics song into a new column called words\n",
    "#list used to store the words\n",
    "words = []\n",
    "\n",
    "#iterate trought each lyric and split unique words appending the result into the words list\n",
    "df1 = df1.reset_index(drop=True)\n",
    "for word in df1['lemmatized_lyrics'].tolist():\n",
    "    words.append(unique(lyrics_to_words(word).split()))\n",
    "    \n",
    "#create the new column with the information of words lists\n",
    "df1['words'] = words\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new dataframe of all the  words used in lyrics and its decades\n",
    "#list used to store the information\n",
    "set_words = []\n",
    "set_year = []\n",
    "\n",
    "#Iterate trought each word and decade and stores them into the new lists\n",
    "for i in df1.index:\n",
    "    for word in df1['words'].iloc[i]:\n",
    "        set_words.append(word)\n",
    "        set_year.append(df1['Year'].iloc[i])\n",
    "#create the new data frame  with the information of words and decade lists\n",
    "words_df1 = pd.DataFrame({'words':set_words,'Year':set_year})\n",
    "\n",
    "# count the frequency of each word that aren't on the stop_words lists\n",
    "cv = CountVectorizer()\n",
    "\n",
    "#Create a dataframe called data_cv to store the the number of times the word was used in  a lyric based their decades\n",
    "text_cv = cv.fit_transform(words_df1['words'].iloc[:])\n",
    "data_cv = pd.DataFrame(text_cv.toarray(),columns=cv.get_feature_names())\n",
    "data_cv['Year'] = words_df1['Year']\n",
    "\n",
    "#created a dataframe that Sums the ocurrence frequency of each word and group the result by decade\n",
    "vect_words = data_cv.groupby('Year').sum().T\n",
    "vect_words = vect_words.reset_index(level=0).rename(columns ={'index':'words'})\n",
    "vect_words = vect_words.rename_axis(columns='')\n",
    "\n",
    "#Save the data into a csv file\n",
    "vect_words.to_csv(os.path.join(export_path, 'words2018.csv'), index=False)\n",
    "vect_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating unique words and examining them (Year 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stores unique words of each lyrics song into a new column called words\n",
    "#list used to store the words\n",
    "words = []\n",
    "\n",
    "#iterate trought each lyric and split unique words appending the result into the words list\n",
    "df2 = df2.reset_index(drop=True)\n",
    "for word in df2['lemmatized_lyrics'].tolist():\n",
    "    words.append(unique(lyrics_to_words(word).split()))\n",
    "    \n",
    "#create the new column with the information of words lists\n",
    "df2['words'] = words\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new dataframe of all the  words used in lyrics and its decades\n",
    "#list used to store the information\n",
    "set_words = []\n",
    "set_year = []\n",
    "\n",
    "#Iterate trought each word and decade and stores them into the new lists\n",
    "for i in df2.index:\n",
    "    for word in df2['words'].iloc[i]:\n",
    "        set_words.append(word)\n",
    "        set_year.append(df2['Year'].iloc[i])\n",
    "        \n",
    "#create the new data frame  with the information of words and decade lists\n",
    "words_df2 = pd.DataFrame({'words':set_words,'Year':set_year})\n",
    "\n",
    "# count the frequency of each word that aren't on the stop_words lists\n",
    "cv = CountVectorizer()\n",
    "\n",
    "#Create a dataframe called data_cv to store the the number of times the word was used in  a lyric based their decades\n",
    "text_cv = cv.fit_transform(words_df2['words'].iloc[:])\n",
    "data_cv = pd.DataFrame(text_cv.toarray(),columns=cv.get_feature_names())\n",
    "data_cv['Year'] = words_df2['Year']\n",
    "\n",
    "#created a dataframe that Sums the ocurrence frequency of each word and group the result by decade\n",
    "vect_words = data_cv.groupby('Year').sum().T\n",
    "vect_words = vect_words.reset_index(level=0).rename(columns ={'index':'words'})\n",
    "vect_words = vect_words.rename_axis(columns='')\n",
    "\n",
    "#Save the data into a csv file\n",
    "vect_words.to_csv(os.path.join(export_path, 'words2019.csv'), index=False)\n",
    "vect_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating unique words and examining them (Year 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(list1):\n",
    "   # intilize a null list\n",
    "     unique_list = []\n",
    "   # traverse for all elements\n",
    "     for x in list1:\n",
    "         # check if exists in unique_list or not\n",
    "         if x not in unique_list:\n",
    "              unique_list.append(x)\n",
    "     return unique_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stores unique words of each lyrics song into a new column called words\n",
    "\n",
    "#list used to store the words\n",
    "words = []\n",
    "#iterate trought each lyric and split unique words appending the result into the words list\n",
    "df3 = df3.reset_index(drop=True)\n",
    "for word in df3['lemmatized_lyrics'].tolist():\n",
    "    words.append(unique(lyrics_to_words(word).split()))\n",
    "    \n",
    "#create the new column with the information of words lists\n",
    "df3['words'] = words\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new dataframe of all the  words used in lyrics and its decades\n",
    "\n",
    "#list used to store the information\n",
    "set_words = []\n",
    "set_year = []\n",
    "#Iterate trought each word and decade and stores them into the new lists\n",
    "for i in df3.index:\n",
    "    for word in df3['words'].iloc[i]:\n",
    "        set_words.append(word)\n",
    "        set_year.append(df3['Year'].iloc[i])\n",
    "#create the new data frame  with the information of words and decade lists\n",
    "words_df3 = pd.DataFrame({'words':set_words,'Year':set_year})\n",
    "\n",
    "# count the frequency of each word that aren't on the stop_words lists\n",
    "cv = CountVectorizer()\n",
    "\n",
    "#Create a dataframe called data_cv to store the the number of times the word was used in  a lyric based their decades\n",
    "text_cv = cv.fit_transform(words_df3['words'].iloc[:])\n",
    "data_cv = pd.DataFrame(text_cv.toarray(),columns=cv.get_feature_names())\n",
    "data_cv['Year'] = words_df3['Year']\n",
    "\n",
    "#created a dataframe that Sums the ocurrence frequency of each word and group the result by decade\n",
    "vect_words = data_cv.groupby('Year').sum().T\n",
    "vect_words = vect_words.reset_index(level=0).rename(columns ={'index':'words'})\n",
    "vect_words = vect_words.rename_axis(columns='')\n",
    "\n",
    "#Save the data into a csv file\n",
    "vect_words.to_csv(os.path.join(export_path, 'words2020.csv'), index=False)\n",
    "vect_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating unique words and examining them (Year 2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(list1):\n",
    "   # intilize a null list\n",
    "     unique_list = []\n",
    "   # traverse for all elements\n",
    "     for x in list1:\n",
    "         # check if exists in unique_list or not\n",
    "         if x not in unique_list:\n",
    "              unique_list.append(x)\n",
    "     return unique_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Stores unique words of each lyrics song into a new column called words\n",
    "\n",
    "#list used to store the words\n",
    "words = []\n",
    "#iterate trought each lyric and split unique words appending the result into the words list\n",
    "df4 = df4.reset_index(drop=True)\n",
    "for word in df4['lemmatized_lyrics'].tolist():\n",
    "    words.append(unique(lyrics_to_words(word).split()))\n",
    "    \n",
    "#create the new column with the information of words lists\n",
    "df4['words'] = words\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for you to examine the dataset separately\n",
    "# df.to_csv('SentAna.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new dataframe of all the  words used in lyrics and its decades\n",
    "#list used to store the information\n",
    "set_words = []\n",
    "set_year = []\n",
    "\n",
    "#Iterate trought each word and decade and stores them into the new lists\n",
    "for i in df4.index:\n",
    "    for word in df4['words'].iloc[i]:\n",
    "        set_words.append(word)\n",
    "        set_year.append(df4['Year'].iloc[i])\n",
    "#create the new data frame  with the information of words and decade lists\n",
    "words_df4 = pd.DataFrame({'words':set_words,'Year':set_year})\n",
    "\n",
    "# count the frequency of each word that aren't on the stop_words lists\n",
    "cv = CountVectorizer()\n",
    "#Create a dataframe called data_cv to store the the number of times the word was used in  a lyric based their decades\n",
    "text_cv = cv.fit_transform(words_df4['words'].iloc[:])\n",
    "data_cv = pd.DataFrame(text_cv.toarray(),columns=cv.get_feature_names())\n",
    "data_cv['Year'] = words_df4['Year']\n",
    "\n",
    "#created a dataframe that Sums the ocurrence frequency of each word and group the result by decade\n",
    "vect_words = data_cv.groupby('Year').sum().T\n",
    "vect_words = vect_words.reset_index(level=0).rename(columns ={'index':'words'})\n",
    "vect_words = vect_words.rename_axis(columns='')\n",
    "\n",
    "#Save the data into a csv file\n",
    "vect_words.to_csv(os.path.join(export_path, 'words2021.csv'), index=False)\n",
    "vect_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the unique words dataframe back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "#import data\n",
    "df_unique_2017 = pd.read_csv(os.path.join(export_path, 'words2017.csv'))\n",
    "df_unique_2017.rename(columns={\"2017\":\"Count\"},inplace=True)\n",
    "# df2017.head()\n",
    "\n",
    "df_unique_2018 = pd.read_csv(os.path.join(export_path, 'words2018.csv'))\n",
    "df_unique_2018.rename(columns={\"2018\":\"Count\"},inplace=True)\n",
    "# df2018.head()\n",
    "\n",
    "df_unique_2019 =  pd.read_csv(os.path.join(export_path, 'words2019.csv'))\n",
    "df_unique_2019.rename(columns={\"2019\":\"Count\"},inplace=True)\n",
    "# df2019.head()\n",
    "\n",
    "df_unique_2020 = pd.read_csv(os.path.join(export_path, 'words2020.csv'))\n",
    "df_unique_2020.rename(columns={\"2020\":\"Count\"},inplace=True)\n",
    "# df2020.head()\n",
    "\n",
    "df_unique_2021 = pd.read_csv(os.path.join(export_path, 'words2021.csv'))\n",
    "df_unique_2021.rename(columns={\"2021\":\"Count\"},inplace=True)\n",
    "# df2021.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dfAll =pd.Dataframe()\n",
    "dfAll=pd.concat([df_unique_2017, df_unique_2018,df_unique_2019,df_unique_2020,df_unique_2021])\n",
    "dfAll.dropna(inplace=True)\n",
    "dfAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfbefcovid =df_unique_2017\n",
    "dfbefcovid=pd.concat([df_unique_2017, df_unique_2018,df_unique_2019])\n",
    "dfbefcovid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcovid=pd.concat([df_unique_2020,df_unique_2021])\n",
    "dfcovid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAll.isnull().values.any()\n",
    "dfAll['Count'].astype(int)\n",
    "dfAll[\"Count\"]= dfAll[\"Count\"].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_wordcloud(dfAll,row,col):\n",
    "    wc = WordCloud(background_color=\"white\",colormap=\"Dark2\",max_font_size=100,random_state=15)\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "     \n",
    "    for index, value in enumerate(dfAll.columns[1:]):\n",
    "        top_dict = dict(zip(dfAll['words'].tolist(),dfAll[value].tolist()))\n",
    "        wc.generate_from_frequencies(top_dict)\n",
    "        plt.subplot(row,col,index+1)\n",
    "        plt.imshow(wc,interpolation=\"bilinear\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"{value}\",fontsize=15)\n",
    "plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "plt.show()\n",
    "#Plot the word cloud\n",
    "plot_wordcloud(dfAll,2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot bef covid\n",
    "def plot_wordcloud(dfbefcovid,row,col):\n",
    "    wc = WordCloud(background_color=\"white\",colormap=\"Dark2\",max_font_size=100,random_state=15)\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "     \n",
    "    for index, value in enumerate(dfbefcovid.columns[1:]):\n",
    "        top_dict = dict(zip(dfbefcovid['words'].tolist(),dfbefcovid[value].tolist()))\n",
    "        wc.generate_from_frequencies(top_dict)\n",
    "        plt.subplot(row,col,index+1)\n",
    "        plt.imshow(wc,interpolation=\"bilinear\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"{value}\",fontsize=15)\n",
    "plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "plt.show()\n",
    "#Plot the word cloud\n",
    "plot_wordcloud(dfbefcovid,2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot covid\n",
    "def plot_wordcloud(dfcovid,row,col):\n",
    "    wc = WordCloud(background_color=\"white\",colormap=\"Dark2\",max_font_size=100,random_state=15)\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "     \n",
    "    for index, value in enumerate(dfcovid.columns[1:]):\n",
    "        top_dict = dict(zip(dfcovid['words'].tolist(),dfcovid[value].tolist()))\n",
    "        wc.generate_from_frequencies(top_dict)\n",
    "        plt.subplot(row,col,index+1)\n",
    "        plt.imshow(wc,interpolation=\"bilinear\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"{value}\",fontsize=15)\n",
    "plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "plt.show()\n",
    "#Plot the word cloud\n",
    "plot_wordcloud(dfcovid,2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat([df2017,df2018,df2019,df2020,df2021])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create lists to store the different scores for each word\n",
    "negative = []\n",
    "neutral = []\n",
    "positive = []\n",
    "compound = []\n",
    "\n",
    "#Initialize the model\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "#Iterate for each row of lyrics and append the scores\n",
    "for i in df.index:\n",
    "    scores = sid.polarity_scores(df['lemmatized_lyrics'].iloc[i])\n",
    "    negative.append(scores['neg'])\n",
    "    neutral.append(scores['neu'])\n",
    "    positive.append(scores['pos'])\n",
    "    compound.append(scores['compound'])\n",
    "    \n",
    "#Create 4 columns to the main data frame  for each score\n",
    "df['negative'] = negative\n",
    "df['neutral'] = neutral\n",
    "df['positive'] = positive\n",
    "df['compound'] = compound\n",
    "df = df.iloc[:,1:]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, group in df.groupby('Year'):\n",
    "    plt.scatter(group['positive'],group['negative'],label=name,s=3)\n",
    "    plt.legend(fontsize=10)\n",
    "\n",
    "plt.title(\"Lyrics Sentiments by Year (Overall)\")\n",
    "plt.xlabel('Positive Valence')\n",
    "plt.ylabel('Negative  Valence')\n",
    "plt.figure(figsize=(60, 60))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_df = df.groupby(['Year']).mean()\n",
    "means_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, group in means_df.groupby('Year'):\n",
    "    plt.scatter(group['positive'],group['negative'],label=name)\n",
    "    plt.legend()\n",
    "\n",
    "plt.title(\"Lyrics Sentiments by Year\")\n",
    "plt.xlabel('Positive Valence')\n",
    "plt.ylabel('Negative  Valence')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before Covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bef covid \n",
    "df_befcovid=pd.concat([df2017,df2018,df2019])\n",
    "df_befcovid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create lists to store the different scores for each word\n",
    "negative = []\n",
    "neutral = []\n",
    "positive = []\n",
    "compound = []\n",
    "\n",
    "#Initialize the model\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "#Iterate for each row of lyrics and append the scores\n",
    "for i in df_befcovid.index:\n",
    "    scores = sid.polarity_scores(df_befcovid['lemmatized_lyrics'].iloc[i])\n",
    "    negative.append(scores['neg'])\n",
    "    neutral.append(scores['neu'])\n",
    "    positive.append(scores['pos'])\n",
    "    compound.append(scores['compound'])\n",
    "    \n",
    "#Create 4 columns to the main data frame  for each score\n",
    "df_befcovid['negative'] = negative\n",
    "df_befcovid['neutral'] = neutral\n",
    "df_befcovid['positive'] = positive\n",
    "df_befcovid['compound'] = compound\n",
    "df_befcovid = df_befcovid.iloc[:,1:]\n",
    "df_befcovid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for name, group in df_befcovid.groupby('Year'):\n",
    "    plt.scatter(group['positive'],group['negative'],label=name, s=3)\n",
    "    plt.legend(fontsize=10)\n",
    "\n",
    "plt.title(\"Lyrics Sentiments by Year (2017-2019)\")\n",
    "plt.xlabel('Positive Valence')\n",
    "plt.ylabel('Negative  Valence')\n",
    "plt.figure(figsize=(250, 250))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_df_bef = df_befcovid.groupby(['Year']).mean()\n",
    "means_df_bef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, group in means_df_bef.groupby('Year'):\n",
    "    plt.scatter(group['positive'],group['negative'],label=name)\n",
    "    plt.legend()\n",
    "\n",
    "plt.title(\"Lyrics Sentiments: Scatter plot of Positive Score and Negative Plot for means by Year (2017-2019)\")\n",
    "plt.xlabel('Positive Valence')\n",
    "plt.ylabel('Negative  Valence')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max value for multiple column\n",
    "prediction = df_befcovid[['negative', 'neutral','positive']].idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predictions to dataframe\n",
    "if 'Sentiment' in df_befcovid.columns:\n",
    "    df_befcovid.drop(columns=['Sentiment'],inplace=True)\n",
    "df_befcovid['Sentiment'] = prediction.tolist()\n",
    "\n",
    "# predictions_df\n",
    "df_befcovid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "ax = sns.countplot(x=\"Sentiment\", data=df_befcovid)\n",
    "plt.title(\"Lyrics Sentiments by Year (2017-2019) - Pre-covid\")\n",
    "\n",
    "# show count (+ annotate)\n",
    "for rect in ax.patches:\n",
    "    ax.text (rect.get_x() + rect.get_width()  / 2,rect.get_height()+ 0.75,rect.get_height(),horizontalalignment='center', fontsize = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Positive = df_befcovid[df_befcovid[\"Sentiment\"] == 'positive']\n",
    "number_of_rows_positive = len(Positive)\n",
    "\n",
    "\n",
    "Negative = df_befcovid[df_befcovid[\"Sentiment\"] == 'negative']\n",
    "number_of_rows_negative = len(Negative)\n",
    "\n",
    "Neutral = df_befcovid[df_befcovid[\"Sentiment\"] == 'neutral']\n",
    "number_of_rows_neutral= len(Neutral)\n",
    "\n",
    "print(\"Number of positive rows:\", number_of_rows_positive)\n",
    "print(\"Number of negative rows:\", number_of_rows_negative)\n",
    "print(\"Number of neutral rows:\", number_of_rows_neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = [number_of_rows_positive, number_of_rows_negative,number_of_rows_neutral]\n",
    "\n",
    "mylabels=[\"Positive\",\"Negative\",\"Neutral\"]\n",
    "colors=['#ADD8E6', '#FFC0CB', '#808000']\n",
    "plt.pie(rows, labels = mylabels,colors=colors,startangle = 90,explode=[0.1, 0.1, 0.1],autopct='%1.2f%%')\n",
    "plt.title(\"Lyrics Sentiments by Year in Percentage(2017-2019) - Pre-covid\")\n",
    "plt.legend()\n",
    "plt.axis('equal')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covid \n",
    "df_covid=pd.concat([df2020,df2021])\n",
    "df_covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create lists to store the different scores for each word\n",
    "negative = []\n",
    "neutral = []\n",
    "positive = []\n",
    "compound = []\n",
    "\n",
    "#Initialize the model\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "#Iterate for each row of lyrics and append the scores\n",
    "for i in df_covid.index:\n",
    "    scores = sid.polarity_scores(df_covid['lemmatized_lyrics'].iloc[i])\n",
    "    negative.append(scores['neg'])\n",
    "    neutral.append(scores['neu'])\n",
    "    positive.append(scores['pos'])\n",
    "    compound.append(scores['compound'])\n",
    "    \n",
    "#Create 4 columns to the main data frame  for each score\n",
    "df_covid['negative'] = negative\n",
    "df_covid['neutral'] = neutral\n",
    "df_covid['positive'] = positive\n",
    "df_covid['compound'] = compound\n",
    "df_covid = df_covid.iloc[:,1:]\n",
    "df_covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, group in df_covid.groupby('Year'):\n",
    "    plt.scatter(group['positive'],group['negative'],label=name, s=3)\n",
    "    plt.legend(fontsize=10)\n",
    "\n",
    "plt.title(\"Lyrics Sentiments by Year (2020-2021) - Covid\")\n",
    "plt.xlabel('Positive Valence')\n",
    "plt.ylabel('Negative  Valence')\n",
    "plt.figure(figsize=(250, 250))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_df_covid = df_covid.groupby(['Year']).mean()\n",
    "means_df_covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for name, group in means_df_covid.groupby('Year'):\n",
    "    plt.scatter(group['positive'],group['negative'],label=name)\n",
    "    plt.legend()\n",
    "\n",
    "plt.title(\"Lyrics Sentiments: Scatter plot of Positive Score and Negative Plot for means by Year (2020-2021)\")\n",
    "plt.xlabel('Positive Valence')\n",
    "plt.ylabel('Negative  Valence')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max value for multiple column\n",
    "predictions = df_covid[['negative', 'neutral','positive']].idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predictions to dataframe\n",
    "if 'Sentiment' in df_covid.columns:\n",
    "    df_covid.drop(columns=['Sentiment'],inplace=True)\n",
    "df_covid['Sentiment'] = predictions.tolist()\n",
    "\n",
    "# predictions_df\n",
    "df_covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "ax = sns.countplot(x=\"Sentiment\", data=df_covid)\n",
    "plt.title(\"Lyrics Sentiments by Year (2020-2021) - Covid\")\n",
    "\n",
    "# show count (+ annotate)\n",
    "for rect in ax.patches:\n",
    "    ax.text (rect.get_x() + rect.get_width()  / 2,rect.get_height()+ 0.75,rect.get_height(),horizontalalignment='center', fontsize = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Positive = df_covid[df_covid[\"Sentiment\"] == 'positive']\n",
    "number_of_rows_positive = len(Positive)\n",
    "\n",
    "\n",
    "Negative = df_covid[df_covid[\"Sentiment\"] == 'negative']\n",
    "number_of_rows_negative = len(Negative)\n",
    "\n",
    "Neutral = df_covid[df_covid[\"Sentiment\"] == 'neutral']\n",
    "number_of_rows_neutral= len(Neutral)\n",
    "\n",
    "print(\"Number of positive rows:\", number_of_rows_positive)\n",
    "print(\"Number of negative rows:\", number_of_rows_negative)\n",
    "print(\"Number of neutral rows:\", number_of_rows_neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rows = [number_of_rows_positive, number_of_rows_negative,number_of_rows_neutral]\n",
    "\n",
    "mylabels=[\"Positive\",\"Negative\",\"Neutral\"]\n",
    "colors=['#ADD8E6', '#FFC0CB', '#808000']\n",
    "plt.pie(rows, labels = mylabels,colors=colors,startangle = 90,explode=[0.1, 0.1, 0.1],autopct='%1.2f%%')\n",
    "plt.legend()\n",
    "plt.title(\"Lyrics Sentiments by Year in Percentage (2020-2021) - Covid\")\n",
    "plt.axis('equal')\n",
    "plt.show() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
